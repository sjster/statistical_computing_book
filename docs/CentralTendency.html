
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Distributions, Central Tendency, and Shape Parameters &#8212; Introduction to Computational Statistics with PyMC3</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Parameter Estimation" href="ParameterEstimation.html" />
    <link rel="prev" title="Manipulating Probability" href="ManipulatingProbability.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo_large.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Introduction to Computational Statistics with PyMC3</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="About.html">
   The What, Why and Whom…
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Databricks.html">
   Introduction to the Databricks Environment
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Bayesian Statistics
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Foundations.html">
   Foundations of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ManipulatingProbability.html">
   Manipulating Probability
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Distributions, Central Tendency, and Shape Parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ParameterEstimation.html">
   Parameter Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Bayes.html">
   Introduction to the Bayes Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Decisions.html">
   Inference and Decisions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Priors.html">
   Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   Bayesian vs. Frequentist Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Distributions.html">
   Introduction to Common Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Sampling.html">
   Sampling Algorithms
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Monte Carlo Methods
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianInference.html">
   Topics in Model Performance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MonteCarlo.html">
   Introduction to Monte Carlo Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  PyMC3 for Bayesian Modeling and Inference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="PyMC3.html">
   Introduction to PyMC3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Reparameterization.html">
   Centered vs. Non-centered Parameterization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Covid_modeling.html">
   Covid Modeling with PyMC3
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/CentralTendency.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sjster/statistical_computing_book/master?urlpath=tree/mini_book/docs/CentralTendency.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sjster/statistical_computing_book/blob/master/mini_book/docs/CentralTendency.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-distributions">
   Probability distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discrete-random-variables">
   Discrete random variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#continuous-random-variables">
   Continuous random variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#moments-expectations-and-variances">
   Moments, expectations and variances
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation">
   Expectation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance-skewness-kurtosis">
   Variance, skewness, kurtosis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties-of-mean-and-variance">
   Properties of mean and variance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-measures">
   Other measures
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#joint-distributions">
   Joint distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marginal-distribution">
     Marginal distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-distributions">
     Conditional distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independence">
     Independence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#covariance">
     Covariance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#correlation-correlation-coefficient">
     Correlation (correlation coefficient)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ungraded-evaluation-20-min">
   UNGRADED EVALUATION (20 min)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graded-evaluation-15-mins">
   GRADED EVALUATION (15 mins)
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="distributions-central-tendency-and-shape-parameters">
<h1>Distributions, Central Tendency, and Shape Parameters<a class="headerlink" href="#distributions-central-tendency-and-shape-parameters" title="Permalink to this headline">¶</a></h1>
<p>In the last video, we discussed sample spaces, events and probability spaces.  We looked at rules for addition and multiplication of probabilities which follow directly from the Axioms of Probability (Kologomorov Axioms).  Here we will continue and extend that discussion to probability distributions and descriptions of those distributions.</p>
<p>Our topics include:</p>
<ol class="simple">
<li><p>probability distributions (discrete vs continuous, pmf vs pdf).</p></li>
<li><p>moments, expectations and variances.</p></li>
<li><p>joint distributions, expectations and covariances with 2 variables.</p></li>
<li><p>marginals.</p></li>
</ol>
<p>General references:</p>
<ul class="simple">
<li><p>Statistical Inference (9780534243128): Casella, George, Berger, Roger L.</p></li>
<li><p>Probability Theory and Statistical Inference: Empirical Modeling with Observational Data (9781107185142): Spanos, A.</p></li>
<li><p>Bayesian Models: A Statistical Primer for Ecologists (9780691159287): Hobbs, N. Thompson, Hooten, Mevin B.</p></li>
<li><p>A First Course in Bayesian Statistical Methods (0387922997): Hoff, Peter D.</p></li>
</ul>
<br>
<br>
<hr style="border:2px solid blue"> </hr><div class="section" id="probability-distributions">
<h2>Probability distributions<a class="headerlink" href="#probability-distributions" title="Permalink to this headline">¶</a></h2>
<p>As a reminder, last time we had introduced the concept of a probability space as a combination of a sample space, event space, and probability function.  The probability function is a real-valued function mapping events to the interval [0,1].  The probability function adheres to the Axioms of Probability (Kolmogorov Axioms).  These are summarized as:</p>
<ol>
<li><p>the probability of an event is a real number on the interval [0,1]</p>
<div class="math notranslate nohighlight">
\[0 \le P(E) \le 1\]</div>
 <br>
</li>
<li><p>the probability of at least one event occuring is 1</p>
<div class="math notranslate nohighlight">
\[P(S) = 1$$, where S is the sample space  
    &lt;br&gt;
3. countable mutually exclusive sets of events satisfy the following  
    
    $$P(\bigcup_{i=1}^\infty E_i) = \sum_{i=1}^\infty P(E_i)\]</div>
</li>
</ol>
<p>In addition to the probability space, we talked about random variables being (often) real-valued functions mapping outcomes (events) to a measureable space.  In set notation, this looks like:</p>
<div class="math notranslate nohighlight">
\[X: \Omega \to \mathbb{R}\]</div>
<p>Combining the mapping of events with the association of the probability function, we have what we need to study random processes.  This mapping defines the probability given by X in a measureable set as:</p>
<div class="math notranslate nohighlight">
\[P(X \in S) = P({x \in \Omega | X(x) \in S})\]</div>
<p>For example, consider the case of tossing two fair coins where we are interested in the events being {(HH),(TT)} or not.  Our sample space is:</p>
<p>S = {(HH),(TT),(TH),(HT)}</p>
<p>Our event space is:</p>
<div class="math notranslate nohighlight">
\[\mathcal{F} = {S, \bar{S}, E, \bar{E} }, E = {(HH),(TT)}\]</div>
<p>where we have defined X(HH)=X(TT)=1, 0 otherwise, which leads us to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P_X(x) = 
\begin{cases}
    \frac{1}{2}, \text{for x = 1} \\
    \frac{1}{2}, \text{for x = 0} \\
\end{cases}
\end{split}\]</div>
<p>We now have our first random variable which happens to be distributed according to the Bernoulli distribution more generally written as:</p>
<p>\(X \sim Bern(\theta)\), where</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x;\theta) = P(X=x;\theta) =
\begin{cases}
    \theta, \text{for x = 1} \\
    1 - \theta, \text{for x = 0} \\
\end{cases}
\end{split}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[f(x;\theta) = \theta^x(1-\theta)^{(1-x)}\]</div>
<p>For \(\theta = [0,1]\).  Distributions are associated with parameters.  As defined above, we see \(\theta\) as the <strong>parameter</strong> associated with the Bernoulli distribution is the probability of success.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="discrete-random-variables">
<h2>Discrete random variables<a class="headerlink" href="#discrete-random-variables" title="Permalink to this headline">¶</a></h2>
<p>As above, discrete random variables are characterized by countable outcome spaces.  Discrete random variables are associated with a <strong>probability mass function (pmf)</strong> whose range is a countable subset of \(\mathbb{R}\) with probability values in the range [0,1].  Properties of the pmf include:</p>
<ol class="simple">
<li><p>\(f_x(x) \ge 0\), for all \(x \in \mathbb{R}_X\).</p></li>
<li><p>\(\sum_{x \in X} f_x(x) = 1\)</p></li>
<li><p>\(F_X(b) - F_X(a) = \sum_{x=a}^b f(x), a &lt; b, a, b \in \mathbb{R}\)</p></li>
</ol>
<p>The last property involves the cdf (cumulative distribution function).  The cdf is defined such that:</p>
<p>\(F_X(b) = \sum_{x \in [-\infty,b]} f_X(x)))</p>
<p>Which, with some thought, we could come up with the properties of the cdf:</p>
<ol class="simple">
<li><p>non-decreasing: \(F_X(x) \le F_X(y)\), for all \(x \le y\)</p></li>
<li><p>right-continuous: \(\lim_{x \downarrow x_0^+} F_X(x) = F_X(x_0)\)</p></li>
<li><p>positive, with range [0,1]</p></li>
</ol>
<p>For discrete variables, \(F_X(x) = P(X \le x) = \sum_{x_i \le x}f(x_i)\)</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr></div>
<div class="section" id="continuous-random-variables">
<h2>Continuous random variables<a class="headerlink" href="#continuous-random-variables" title="Permalink to this headline">¶</a></h2>
<p>In the discrete case, random variables are characterized by countable outcome spaces where the random variable maps events to discrete values on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>.  How do we deal with outcome spaces that are continuous?  We use intervals.</p>
<p>Continuous random variables are associated with a <strong>probability density function (pdf)</strong> whose range is an uncountable subset of \(\mathbb{R}\) with probability values in the range \([0,\infty]\).  Properties of the pdf include:</p>
<ol class="simple">
<li><p>\(f_x(x) \ge 0\), for all \(x \in \mathbb{R}_X\).</p></li>
<li><p>\(\int_{-\infty}^\infty f_x(x)dx = 1\)</p></li>
<li><p>\(F_X(b) - F_X(a) = \int_{a}^b f(x)dx,\ a &lt; b, (a, b) \in \mathbb{R}\)</p></li>
</ol>
<p>Note, the cdf of a continuous random variable is generally defined as:
\(P(X \le x) = F_X(x) = \int_{-\infty}^x f(u)du\) where we can we can recover the pdf via \(f(x) = \frac{dF(x)}{dx}\).  The properties of the continuous cdf are obvious extensions of the discrete case.  One difference we sometimes overlook is the <span class="math notranslate nohighlight">\(P(X=x)=0\)</span> AND in the continuous case because \(x\) is a point value.  To solve this conundrum, we we use intervals, ie area under the curve when talking probabilities.</p>
<p>Arguably, the most widely studied and used distribution in statistics is the normal distribution.  The pdf of the normal distribution is given as</p>
<p>\(X \sim Norm(\mu,\sigma^2)\) , where</p>
<div class="math notranslate nohighlight">
\[f_x(x,\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, \text{ where } (x,\mu,\sigma^2) \in (\mathbb{R},\mathbb{R},\mathbb{R}_+)\]</div>
<p>A plot of the normal pdf is show below. Where we are setting \(\mu = 0\) and \(\sigma^2=1\).  Note, the <strong>parameters</strong> of the distribution as given are \(\mu\) and \(\sigma^2\).  The parameters of the normal distribution describe the location and shape: \(\mu\) gives the center of mass while \(\sigma^2\) gives and indication of spread.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">variance</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="c1">#bounds and granularity</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/CentralTendency_4_0.png" src="../_images/CentralTendency_4_0.png" />
</div>
</div>
</div>
<div class="section" id="moments-expectations-and-variances">
<h2>Moments, expectations and variances<a class="headerlink" href="#moments-expectations-and-variances" title="Permalink to this headline">¶</a></h2>
<p>Distributions are recognizable by characteristics such as range, location and shape.  Computing moments such as expectation, variance, skewness and kurtosis can be used to estimate the parameters that define a distribution.</p>
</div>
<div class="section" id="expectation">
<h2>Expectation<a class="headerlink" href="#expectation" title="Permalink to this headline">¶</a></h2>
<p>The expectation of a function is the average value of the function under a probability distribution.  In the case of discrete distributions, this is computed as the weighted average where the weights are dictated by the probability at the value of x (where p(x) is the pmf/pdf):</p>
<div class="math notranslate nohighlight">
\[E[f] = \sum_x f(x)^r p(x)\]</div>
<p>For continuous distributions, this looks like</p>
<div class="math notranslate nohighlight">
\[E[f] = \int f(x)^r p(x) dx.\]</div>
<p>if \(f(x) = x\) and \(r=1\) in both cases (discrete and continuous), this is called the mean of the distribution.  In the figure of the normal distribution above, computing the expectation will give \(E[x]=0\) which matches our intuition of where the bulk of the mass is centered based on the figure.</p>
<p>For the Bernoulli distribution given earlier, we compute this as</p>
<div class="math notranslate nohighlight">
\[E[x] = \sum_{x=0,1} x\ast p(x,\theta) = 0\ast(1-\theta) + 1\ast\theta = \theta\]</div>
<p>Thus, for the Bernoulli distribution, the mean is equal to the parameter for success, \(E[x]_{Bern} = \theta\).</p>
</div>
<div class="section" id="variance-skewness-kurtosis">
<h2>Variance, skewness, kurtosis<a class="headerlink" href="#variance-skewness-kurtosis" title="Permalink to this headline">¶</a></h2>
<p>While the mean of the distribution, also the first raw moment, is given by the first power of \(x^1\), other interesting moments arrise when we take powers of \(X-E[X]\).</p>
<p>Variance, measures spread of the distribution:<br />
\(Var(X) = E[(X-E[X])^2] = \int (X-E[X])^2 p(x) dx\) in the continuous case.</p>
<p>Skewness, measures symmetry:<br />
$<span class="math notranslate nohighlight">\(\alpha_3 = \frac{E(X-E[X])^3}{(\sqrt{Var(X)})^3}\)</span>$</p>
<p>Kurtosis, measures “peakiness”:<br />
$<span class="math notranslate nohighlight">\(\alpha_4 = \frac{E(X-E[X])^4}{(\sqrt{Var(X)})^4}\)</span>$</p>
</div>
<div class="section" id="properties-of-mean-and-variance">
<h2>Properties of mean and variance<a class="headerlink" href="#properties-of-mean-and-variance" title="Permalink to this headline">¶</a></h2>
<p>Expectation is a linear operator.  As such, we can easily prove the following useful properties:</p>
<ol class="simple">
<li><p>\(E[c] = c\), where c is a constant.</p></li>
<li><p>\(E[aX_1 + bX_2] = aE[X_1] + bE[X_2]\)</p></li>
</ol>
<p>Similarly, with variance:</p>
<ol class="simple">
<li><p>\(Var(X) = E[(X-E[X])^2] = E[X^2] - (E[X])^2\)</p></li>
<li><p>\(Var(c) = 0\), where c is a constant.</p></li>
<li><p>\(Var(aX_1 + bX_2) = a^2Var(X_1) + b^2Var(X_2)\)</p></li>
</ol>
<p>Standard deviation is defined as \(\sigma = \sqrt{Var(X)}\).  This is a useful quantity as it is on the same scale as X.</p>
</div>
<div class="section" id="other-measures">
<h2>Other measures<a class="headerlink" href="#other-measures" title="Permalink to this headline">¶</a></h2>
<p>Mode – most frequent value.
\(\frac{df(x)}{dx}=0\) needs to be max.</p>
<p>Median – middle value, place where probability is equal to right/left.
\(P(X &lt; x) = P(X &gt; x) = 1/2\)</p>
<p>Quantiles are defined using the cdf.
\(F_X(x) \ge p\)</p>
<p>For the normal distribution, mean = median = mode.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr></div>
<div class="section" id="joint-distributions">
<h2>Joint distributions<a class="headerlink" href="#joint-distributions" title="Permalink to this headline">¶</a></h2>
<p>In many cases, we will be working on distributions of more than one variable.  These are termed joint distributions.  These joint distributions can be of mixed type, e.g. Bernoulli and normal, normal and Cauchy, etc.  These can be thought of in the normal way:</p>
<div class="math notranslate nohighlight">
\[P_{XY}(x,y) = P(X=x,Y=y)\]</div>
<p>We often want to determine a parameter in a distribution of more than one variable.  For instance, the expectation of \(f(x,y)\)</p>
<div class="math notranslate nohighlight">
\[E[f(x,y)] = \int_{x,y} f(x,y) p(x,y) dxdy\]</div>
<p>where \(f(x,y)=xy\) would give the mean analogous to the univariate case.</p>
<div class="section" id="marginal-distribution">
<h3>Marginal distribution<a class="headerlink" href="#marginal-distribution" title="Permalink to this headline">¶</a></h3>
<p>In cases where we have a joint distribution, we will sometimes need to find the marginal distribution.  The marginial distribution for a case where we have a joint distribution of X and Y is:</p>
<p>\(P_X(x) = \sum_{all y_i} P_{XY}(x,y_i)\) and similarly for the marginal in Y.  The extension to a continuous case or mixed case is an obvious use of an integral.</p>
<p>Intuitively, we are integrating (or averaging) out the effect of R.V. Y to get at the marginal distribution of X.</p>
</div>
<div class="section" id="conditional-distributions">
<h3>Conditional distributions<a class="headerlink" href="#conditional-distributions" title="Permalink to this headline">¶</a></h3>
<p>Remember our rules of probabillity …</p>
<div class="math notranslate nohighlight">
\[P_{XY}(x_i|y_j) = \frac{P_{XY}(x_i,y_j)}{P_Y(y_i)}\]</div>
<p>The conditional expectation is given by</p>
<div class="math notranslate nohighlight">
\[E[X|Y=y_j] = \sum_{X} x_iP_{X|Y}(x_i|y_j)\]</div>
<p>or in the continuous case for a function g:</p>
<div class="math notranslate nohighlight">
\[E[g(Y)|x] = \int_{-\infty}^{\infty} g(y)f(y|x)dy\]</div>
</div>
<div class="section" id="independence">
<h3>Independence<a class="headerlink" href="#independence" title="Permalink to this headline">¶</a></h3>
<p>It can be shown that if X and Y are independent, there exists some functions g(x) and h(y) such that:</p>
<p>\(f(x,y) = g(x)h(y)\) for all (x,y)</p>
<p>How do we use this?  In the discrete case, if we can find a pair (x,y) that violate the product rule, the random variables are dependent.</p>
</div>
<div class="section" id="covariance">
<h3>Covariance<a class="headerlink" href="#covariance" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[cov[x,y] = E[(x-E[x])(y-E[y])] = E_{x,y}[xy]-E_x[x]E_y[y]\]</div>
</div>
<div class="section" id="correlation-correlation-coefficient">
<h3>Correlation (correlation coefficient)<a class="headerlink" href="#correlation-correlation-coefficient" title="Permalink to this headline">¶</a></h3>
<p>The correlation coefficient gives an indication of how much or little X,Y vary together.  If X,Y are independent, \(cov(X,Y)= corr(X,Y) = 0\).  Correlary is not true.</p>
<div class="math notranslate nohighlight">
\[\rho_{XY} = \frac{Cov(X,Y)}{\sigma_x\sigma_y}\]</div>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
</div>
<div class="section" id="ungraded-evaluation-20-min">
<h2>UNGRADED EVALUATION (20 min)<a class="headerlink" href="#ungraded-evaluation-20-min" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>Find the CDF of a distribution, given by \(X\sim exp(\lambda)\) where the pdf: \(f(x|\lambda) = \lambda e^{-\lambda x}, x,\lambda \in \mathbb{R}_+,\mathbb{R}&gt;0\)</p>
<p>a. \(1-e^{(-\lambda x)}\) (C)<br />
b. \(1+e^{(-\lambda x)}\)</p>
</li>
<li><p>Find the expection of a function</p>
<p>Assuming \(X \sim exp(\lambda)\), find the expection of \(g(x)=x^2\).</p>
<p>a. \(\frac{1}{\lambda^2}\)</p>
<p>b. \(\frac{2}{\lambda^2}\) (C)</p>
</li>
<li><p>Correlation of X and Y equals 0.  Are X and Y independent?</p>
<p>a. yes</p>
<p>b. can’t tell (C)</p>
</li>
<li><p>Expectation is</p>
<p>a. what we expect to happen all the time<br />
b. a weighted average of possible outcomes</p>
</li>
<li><p>The mean, or first raw moment, of a discrete random variable is defined as:</p>
<p>a. E[X] = \(\sum_{x \in X} x p(x)\)  (C)<br />
b. E[f(x)] = \(\sum_{x \in X} f(x) p(x)\), where p(x) = x and f(x) = x-1</p>
</li>
<li><p>You computed the covariance of two random variables to be 0.  These RV’s are therefor independent.</p>
<p>a. true<br />
b. false  (C)</p>
</li>
<li><p>For the RV in the last question, find the expectation of x.</p>
<p>a. \( \frac{1}{\lambda^2}\)<br />
a. \( \frac{2}{\lambda^2}\) (C)</p>
</li>
<li><p>Are X and Y independent when the joint pdf is defined as \(f(x,y) = \frac{1}{57}xy^6e^{-y-x},x&gt;0,y&gt;0\)</p>
<p>a. True  (C)</p>
<p>b. False</p>
</li>
</ol>
</div>
<div class="section" id="graded-evaluation-15-mins">
<h2>GRADED EVALUATION (15 mins)<a class="headerlink" href="#graded-evaluation-15-mins" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>Two events A and B are independent, thier joint probability is given as?</p>
<p>a. P(A,B) = P(A) * P(B)  (C)</p>
<p>b. P(A,B) = 0</p>
</li>
<li><p>Given both a fair coin and 6-side die, we know P(coin) = 1/2 and P(die) = 1/6 for any landing of the coin and die.  What is the joint probability for any of the possible combinations of outcomes?</p>
<p>a. 1/12  (C)<br />
b. 1/3</p>
</li>
<li><p>In a certain game, you are given P(A,B) = 1/12, P(A) = 1/2 and P(B) = 1/4.  Are the events independent?</p>
<p>a. no  (C)<br />
b. yes</p>
</li>
<li><p>For the situation in the last question, what is P(A|B)?</p>
<p>a. P(A|B) = P(A,B)/P(B)  (C)<br />
b. 0</p>
</li>
<li><p>Two events are said to be exchangeable if:</p>
<p>a. The order of occurance doesn’t matter (C)<br />
b. They are independent</p>
</li>
<li><p>The marginal distribution of X given the joint distribution as \(f(x,y) = 4xy; x,y \in [0,1]\) is</p>
<p>a. A function of x (C)</p>
<p>b. A function of y</p>
</li>
<li><p>Find the marginal distribution of x in \(f(x,y)\) from the last problem.</p>
<p>a. 4x (C)</p>
<p>b. 4y</p>
</li>
<li><p>A random variable is</p>
<p>a. A real valued function mapping occurance of events to the real interval [0,1]<br />
b. A real valued function mapping outcomes to the real number line (C)</p>
</li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sjster/statistical_computing_book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="ManipulatingProbability.html" title="previous page">Manipulating Probability</a>
    <a class='right-next' id="next-link" href="ParameterEstimation.html" title="next page">Parameter Estimation</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Srijith Rajamohan, Ph.D.<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>